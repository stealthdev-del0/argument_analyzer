# ğŸ—ï¸ Architecture Guide - Argument Structure Analyzer

## System Design Overview

Das **Argument Structure Analyzer** Projekt folgt einer modularen **NLP-Pipeline-Architektur** mit klarer Separation of Concerns. Jedes Modul hat eine spezifische Aufgabe und ist unabhÃ¤ngig testbar.

---

## ğŸ“Š Dataflow Diagram

```
Input Text (beliebiger Text)
    â†“
[TextPreprocessor]  â”€â”€â†’  Sentences + Tokens
    â†“
[ClaimDetector]     â”€â”€â†’  ClaimResult (Type, Confidence)
    â†“                      â†“
[EmotionAnalyzer]   â”€â”€â†’  EmotionResult (Sentiment, Score)
    â†“
[ArgumentClassifier] â”€â”€â†’ ArgumentClassification (vollstÃ¤ndig klassifiziert)
    â†“
[StructureBuilder]   â”€â”€â†’ ArgumentNodes (Baum-Struktur)
    â†“
[TerminalVisualizer] â”€â”€â†’ ğŸ“Š Terminal Output
```

---

## ğŸ” Module Details

### 1. **preprocessing.py** - Satzsegmentierung & Tokenisierung
**Zweck:** Text in SÃ¤tze und Tokens zerlegen

**Datenstrukturen:**
```python
@dataclass
class Token:
    text: str          # "climate"
    pos: str           # "NOUN"
    lemma: str         # "climate"
    dep: str           # "nsubj" (dependency)

@dataclass
class Sentence:
    text: str          # "Climate change is real."
    tokens: List[Token]
    doc_id: int        # Satz-Index
```

**Funktionen:**
- `process_text(text)` â†’ `List[Sentence]`
- `extract_verbs(sentence)` â†’ `List[str]` (Lemmatisierung)
- `extract_entities(text)` â†’ `List[Tuple[str, str]]` (Named Entities)

**Fallback-Tokenizer:** Wenn spaCy nicht verfÃ¼gbar, nutzt Regex-basierte Segmentierung

---

### 2. **claim_detection.py** - Thesis & Argument Type Detection
**Zweck:** Klassifizieren von SÃ¤tzen nach Argumenttyp

**Datenstruktur:**
```python
@dataclass
class ClaimResult:
    sentence_text: str
    confidence: float  # 0.0 - 1.0
    markers: List[str] # ["therefore", "should"]
    claim_type: str    # "CLAIM", "SUPPORT", "COUNTER", "NEUTRAL"
```

**Heuristiken (MVP):**
- **CLAIM:** "therefore", "thus", "conclude", "should", "must"  
  Confidence: ~0.9
- **SUPPORT:** "because", "since", "evidence", "research shows"  
  Confidence: ~0.85
- **COUNTER:** "however", "but", "although", "critics say"  
  Confidence: ~0.85
- **NEUTRAL:** Keine Marker gefunden
  Confidence: 0.0

**Algorithmus:**
```
1. Konvertiere Satz zu Lowercase
2. Suche nach Markern pro Type
3. Berechne durchschnittliche Confidence pro Type
4. WÃ¤hle Type mit hÃ¶chster Confidence
```

---

### 3. **emotion_analysis.py** - Sentiment & Emotionality Detection
**Zweck:** Emotionale Sprache & Sentiment analysieren

**Datenstruktur:**
```python
@dataclass
class EmotionResult:
    sentence_text: str
    sentiment: str         # "positive", "negative", "neutral"
    sentiment_score: float # -1.0 bis +1.0
    emotionality: float    # 0.0 bis 1.0 (wie emotional)
    emotion_keywords: List[str]
```

**Sentiment-Berechnung:**
```
sentiment_score = (positive_words - negative_words) / (positive_words + negative_words)

Wenn sentiment_score > 0.1  â†’ "positive"
Wenn sentiment_score < -0.1 â†’ "negative"
Sonst                       â†’ "neutral"
```

**Emotionality** = Min(1.0, (positive_count + negative_count) / 5.0)

**Emotionale Marker:**
- Positive: "good", "great", "excellent", "love", "benefit"
- Negative: "bad", "terrible", "hate", "wrong", "stupid"
- Intensifier: "very" (+1.2x), "extremely" (+1.3x), "absolutely" (+1.3x)
- CAPS_LOCK: GroÃŸbuchstaben â†’ +0.3 Score
- Ausrufezeichen: ! â†’ +0.2 Score pro !

---

### 4. **argument_classification.py** - Kombinierte Klassifikation
**Zweck:** Claims + Emotions zusammenbringen + Argument-StÃ¤rke berechnen

**Datenstruktur:**
```python
@dataclass
class ArgumentClassification:
    sentence_text: str
    argument_type: str     # "CLAIM", "SUPPORT", "COUNTER", "NEUTRAL"
    confidence: float      # Aus ClaimDetector
    sentiment: str         # Aus EmotionAnalyzer
    emotionality: float    # Aus EmotionAnalyzer
    keywords: List[str]    # Marker + Emotion Keywords
    strength: float        # 0.0 - 1.0 (kombiniert)
```

**Strength-Berechnung:**
```
strength = 
    0.5 * confidence +           # Claim-Confidence ist wichtig
    0.3 * emotionality_factor +  # Moderate EmotionalitÃ¤t besser
    0.2 * sentiment_factor       # Sentiment-Konsistenz
```

**Logical Weakness Detection:**
- âŒ Ad-Hominem: "stupid", "idiot", "fool"
- âš ï¸ Overgeneralization: "all", "never", "always", "everybody"
- âš ï¸ Over-emotional: emotionality > 0.7
- âš ï¸ Superlatives: zu viele "absolutely", "definitely"
- âš ï¸ Circular reasoning: zu viele "is" Aussagen

---

### 5. **structure_builder.py** - Argument-Graph Konstruktion
**Zweck:** Baut Baum-Struktur aus klassifizierten Argumenten

**Datenstruktur:**
```python
@dataclass
class ArgumentNode:
    id: int
    text: str
    arg_type: str       # "CLAIM", "SUPPORT", "COUNTER", "NEUTRAL"
    strength: float
    emotionality: float
    children: List['ArgumentNode']
    parent: Optional['ArgumentNode']
```

**Struktur-Regeln (MVP):**
1. Identifiziere alle **CLAIM**-Nodes als Roots
2. **SUPPORT** & **COUNTER** Nodes werden als Children assigned
3. Zuordnung nach Position im Original-Text (naive)

**Beispiel:**
```
ğŸŸ¢ CLAIM (Root)
 â”œâ”€â”€ ğŸ”µ SUPPORT
 â”œâ”€â”€ ğŸ”µ SUPPORT
 â””â”€â”€ ğŸŸ£ COUNTER
      â””â”€â”€ (impliziter Rebuttal)
```

**Methoden:**
- `build_structure()` â†’ `List[ArgumentNode]` (Root Claims)
- `get_argument_tree_dict()` â†’ Dictionary (fÃ¼r Visualisierung)
- `get_tree_stats()` â†’ Statistiken (depth, count, avg_strength)
- `visualize_ascii()` â†’ ASCII-Baum-String
- `get_strongest_path()` â†’ Linear Path der stÃ¤rksten Argumente

---

### 6. **visualizer.py** - Terminal-Ausgaben
**Zweck:** SchÃ¶ne, lesbare Terminal-Visualisierung

**Output-Komponenten:**

1. **Argument Analysis** - Detaillierte Auflistung mit:
   - Argument Type (Icon: ğŸŸ¢ğŸ”µğŸŸ£âšª)
   - Confidence Bar
   - Strength Bar
   - Emotionality Bar + Sentiment Icon
   - Keywords

2. **Argument Summary** - Grouped Statistics:
   - Count pro Type
   - Avg Strength
   - Beispiele

3. **Structure Visualization** - ASCII-Baum:
   ```
   â””â”€â”€ ğŸŸ¢ [CLAIM] Main thesis...
       â”œâ”€â”€ ğŸ”µ [SUPPORT] Supporting argument...
       â””â”€â”€ ğŸŸ£ [COUNTER] Counter argument...
   ```

4. **Emotional Analysis** - Sentiment Ãœbersicht:
   - Positive/Negative/Neutral Count
   - Avg Sentiment Score
   - Avg Emotionality

5. **Logical Weaknesses** - Erkannte Fallacies

6. **Strongest Arguments** - Top 3 nach Strength

---

### 7. **main.py** - Entry Point & CLI
**Zweck:** Benutzerinteraktion & Pipeline-Orchestrierung

**Funktions-Modi:**
```bash
python main.py                    # Demo mit Beispiel-Text
python main.py -i                 # Interaktiv
python main.py -f essay.txt       # Datei-Modus
python main.py -h                 # Help
python main.py "Custom text"      # Direkt Text
```

**Workflow:**
1. Parse Eingabe (CLI Argumente oder interaktiv)
2. Rufe `main(text)` auf
3. Orchestriere alle Module:
   - TextPreprocessor
   - ClaimDetector
   - EmotionAnalyzer
   - ArgumentClassifier
   - StructureBuilder
4. Nutze TerminalVisualizer fÃ¼r Output
5. Return Ergebnisse als Dictionary

---

## ğŸ”„ Component Interaction

```
main.py
  â”‚
  â”œâ”€â†’ preprocessing.TextPreprocessor()
  â”‚     â”‚ process_text(text)
  â”‚     â””â”€ Sentences[] 
  â”‚
  â”œâ”€â†’ claim_detection.ClaimDetector()
  â”‚     â”‚ detect_claims(sentences)
  â”‚     â””â”€ ClaimResult[]
  â”‚
  â”œâ”€â†’ emotion_analysis.EmotionAnalyzer()
  â”‚     â”‚ analyze_emotions(sentences)
  â”‚     â””â”€ EmotionResult[]
  â”‚
  â”œâ”€â†’ argument_classification.ArgumentClassifier()
  â”‚     â”‚ classify_arguments(sentences)
  â”‚     â”‚ get_argument_summary()
  â”‚     â”‚ detect_logical_weaknesses()
  â”‚     â””â”€ ArgumentClassification[]
  â”‚
  â”œâ”€â†’ structure_builder.StructureBuilder()
  â”‚     â”‚ build_structure(classifications)
  â”‚     â”‚ get_tree_stats()
  â”‚     â”‚ visualize_ascii()
  â”‚     â””â”€ ArgumentNode[]
  â”‚
  â””â”€â†’ visualizer.TerminalVisualizer()
        â””â”€ print_full_analysis()
```

---

## ğŸ§ª Testing Strategy

**Unit Tests** (`test_units.py`):
- `TestPreprocessing`: Text â†’ Sentences
- `TestClaimDetection`: Marker Detection
- `TestEmotionAnalysis`: Sentiment/Emotionality
- `TestArgumentClassification`: Strength Calculation
- `TestIntegration`: Full Pipeline

**Test Cases** (`test_cases.py`):
- climate_change
- ai_ethics
- education
- gun_control
- social_media

---

## ğŸ“ˆ Performance Characteristics

| Component | Complexity | Speed |
|-----------|-----------|-------|
| Preprocessing | O(n) | < 10ms |
| Claim Detection | O(n*m) | < 50ms |
| Emotion Analysis | O(n*k) | < 30ms |
| Classification | O(n) | < 10ms |
| Structure Building | O(nÂ²) | < 30ms |
| Visualization | O(n) | < 20ms |
| **Total** | **O(nÂ²)** | **~150ms** |

(n = # sentences, m = # markers, k = # emotion words)

---

## ğŸ¯ Design Patterns

1. **Dataclass Pattern** - Verwendung fÃ¼r strukturierte Datentypen
2. **Pipeline Pattern** - Module verarbeiten Input â†’ Output sequentiell
3. **Strategy Pattern** - Fallback-Tokenizer wenn spaCy nicht verfÃ¼gbar
4. **Decorator Pattern** - ASCII-Codes fÃ¼r Terminal-Icons (ğŸŸ¢ğŸ”µğŸŸ£)

---

## ğŸ”® Future Architecture (Level 2+)

### Level 2 - Embeddings & Clustering
```
Arguments
    â†“
[Vectorizer: Word2Vec/GloVe]
    â†“
Embeddings (768-dim)
    â†“
[Clusterer: K-Means]
    â†“
Argument Clusters
    â†“
[GraphViz Renderer]
    â†“
ğŸ’» Interactive Graph Visualization
```

### Level 3 - Transformer-basiert
```
Text
    â†“
[BERT Tokenizer]
    â†“
[Fine-tuned BERT Encoder]
    â†“
[Claim Classification Head]
[Relation Classification Head]
    â†“
Confidence Scores + Relations
```

---

## ğŸ“ Design Considerations

### Warum Keyword-Heuristiken? 
- âœ… Fast (kein ML-Training)
- âœ… Interpretierbar (welche Marker gefunden?)
- âœ… Erweiterbar (einfach Keywords hinzufÃ¼gen)
- âš ï¸ Aber: Begrenzte Genauigkeit ohne ML

### Warum Fallback-Tokenizer?
- âœ… Funktioniert ohne externe Dependencies
- âœ… Robuster gegen Python/Library-InkompatibilitÃ¤t
- âš ï¸ Aber: Weniger Features (keine POS-Tags ohne spaCy)

### Warum Satz-basierte Analyse?
- âœ… Klare Einheiten fÃ¼r Klassifikation
- âœ… Einfach zu visualisieren
- âš ï¸ Aber: Kann multi-sentence Arguments verpassen

---

## ğŸ› ï¸ Extension Points

Wo du leicht neue Features hinzufÃ¼gen kannst:

1. **Claim Detection**: Neue Keywords in `CLAIM_MARKERS` dict
2. **Emotion Analysis**: Neue WÃ¶rter in `POSITIVE_WORDS` / `NEGATIVE_WORDS`
3. **Logical Weakness**: Neue Pattern in `detect_logical_weaknesses()`
4. **Output Format**: Neuer Visualizer (z.B. JSON, HTML)
5. **Language Support**: Deutsche Keywords hinzufÃ¼gen

---

**System-KomplexitÃ¤t:** â­â­â­ (Mittel)  
**Erweiterbarkeit:** â­â­â­â­ (Hoch)  
**Production-Readiness:** â­â­ (MVP-Quality)
